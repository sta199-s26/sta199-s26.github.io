---
title: "AE 14: Chicago taxi classification"
categories: 
  - Application exercise
draft: true
---

In this application exercise, we will

-   Split our data into testing and training
-   Fit logistic regression regression models to testing data to classify outcomes
-   Evaluate performance of models on testing data

We will use **tidyverse** and **tidymodels** for data exploration and modeling,

```{r}
#| label: load-packages
#| message: false
library(tidyverse)
library(tidymodels)
```

and the `chicago_taxi` dataset introduced in the lecture.

```{r}
chicago_taxi <- read_csv("data/chicago-taxi.csv") |>
  mutate(
    tip = fct_relevel(tip, "no", "yes"),
    local = fct_relevel(local, "no", "yes"),
    dow = fct_relevel(dow, "Mon", "Tue", "Wed", "Thu", "Fri", "Sat", "Sun"),
    month = fct_relevel(month, "Jan", "Feb", "Mar", "Apr")
  )
```

Remember from the lecture that the `chicago_taxi` dataset contains information on whether a trip resulted in a tip (`yes`) or not (`no`) as well as numerical and categorical features of the trip.

```{r}
#| label: glimpse-chicago-taxi
glimpse(chicago_taxi)
```

# Spending your data

Split your data into testing and training in a reproducible manner and display the split object.

```{r}
#| label: train-test-split
# add code here
```

What percent of the original `chicago_taxi` data is allocated to training and what percent to testing?
Compare your response to your neighbor's.
Are the percentages roughly consistent?
What determines this in the `initial_split()`?
How would the code need to be updated to allocate 80% of the data to training and the remaining 20% to testing?

```{r}
#| label: train-test-percentages
# add code here
```

75% of the data is allocated to training and the remaining 25% to testing.
This is because the `prop` argument in `initial_split()` is `3/4` by default.
The code would need to be updated as follows for a 80%/20% split:

```{r}
#| label: train-test-80-20
# split 80-20
# add code here
```

Let's stick with the default split and save our testing and training data.

```{r}
#| label: train-test-save
# add code here
```

# Model 1: Custom choice of predictors

## Fit

Fit a model for classifying trips as tipped or not based on a subset of predictors of your choice.
Name the model `chicago_taxi_custom_fit` and display a tidy output of the model.

```{r}
#| label: forested-custom-fit
# add code here
```

## Predict

Predict for the testing data using this model.

```{r}
#| label: forested-custom-aug
# add code here
```

## Evaluate

Calculate the false positive and false negative rates for the testing data using this model.

```{r}
#| label: forested-custom-eval
# add code here
```

Another commonly used display of this information is a confusion matrix.
Create this using the `conf_mat()` function.
You will need to review the documentation for the function to determine how to use it.

```{r}
#| label: conf-mat-custom
# add code here
```

## Sensitivity, specificity, ROC curve

Calculate sensitivity and specificity and draw the ROC curve.

```{r}
#| label: forested-custom-roc
# add code here
```

# Model 2: All predictors

## Fit

Fit a model for classifying plots as forested or not based on all predictors available.
Name the model `forested_full_fit` and display a tidy output of the model.

```{r}
#| label: forested-full-fit
# add code here
```

## Predict

Predict for the testing data using this model.

```{r}
#| label: forested-full-aug
# add code here
```

## Evaluate

Calculate the false positive and false negative rates for the testing data using this model.

```{r}
#| label: forested-full-eval
# add code here
```

## Sensitivity, specificity, ROC curve

Calculate sensitivity and specificity and draw the ROC curve.

```{r}
#| label: forested-full-roc
# add code here
```

# Model 1 vs. Model 2

Plot both ROC curves and articulate how you would use them to compare these models. Also calculate the areas under the two curves.

```{r}
#| label: compare
# add code here
```
